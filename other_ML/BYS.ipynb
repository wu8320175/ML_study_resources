{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],                #切分的词条\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]                                                                   #类别标签向量，1代表侮辱性词汇，0代表不是\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "[0, 1, 0, 1, 0, 1]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "postingList,classVec=loadDataSet()\n",
    "print(postingList)\n",
    "print(classVec)\n",
    "data=pd.Series(classVec)\n",
    "print(data.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postingList:\n",
      " [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "myVocabList:\n",
      " ['take', 'not', 'stop', 'licks', 'stupid', 'is', 'him', 'I', 'so', 'mr', 'park', 'love', 'has', 'help', 'garbage', 'food', 'maybe', 'cute', 'please', 'posting', 'dog', 'how', 'problems', 'to', 'dalmation', 'ate', 'my', 'worthless', 'buying', 'steak', 'quit', 'flea']\n",
      "trainMat:\n",
      " [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1], [1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "def createVocabList(dataSet):\n",
    "    #将词汇表转换成不含重复词的表\n",
    "    vocablists_temp=set()\n",
    "    for Vlists in dataSet:\n",
    "        vocablists_temp=vocablists_temp|set(Vlists)\n",
    "    return list(vocablists_temp)\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)                                    #创建一个其中所含元素都为0的向量\n",
    "    for word in inputSet:                                                #遍历每个词条\n",
    "        if word in vocabList:                                            #如果词条存在于词汇表中，则置1\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec                                                    #返回文档向量\n",
    "\n",
    "postingList, classVec = loadDataSet()\n",
    "print('postingList:\\n',postingList)\n",
    "myVocabList = createVocabList(postingList)\n",
    "print('myVocabList:\\n',myVocabList)\n",
    "trainMat = []\n",
    "for postinDoc in postingList:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "print('trainMat:\\n', trainMat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    #将词汇表转换成不含重复词的表\n",
    "    vocablists_temp=set()\n",
    "    for Vlists in dataSet:\n",
    "        vocablists_temp=vocablists_temp|set(Vlists)\n",
    "    return list(vocablists_temp)\n",
    "mylist=createVocabList(postingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#生成非重复词汇表长度的向量，原语句中单词在非重复词汇表中对应向量的位置设置为1其他为0\n",
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    vocabList_temp=[]\n",
    "    for word in inputSet: \n",
    "        if word in vocabList:\n",
    "            vocabList_temp.append(1)\n",
    "        else:\n",
    "            vocabList_temp.append(0)\n",
    "    return vocabList_temp\n",
    "\n",
    "trainSet=[]\n",
    "for postingitem in postingList:\n",
    "    trainSet.append(setOfWords2Vec(postingitem,mylist))\n",
    "arr1=np.array(trainSet)\n",
    "arr1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[1. 1. 1. 0. 3. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 2. 0. 0. 1.\n",
      " 0. 0. 0. 2. 1. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.        , 0.04166667, 0.04166667, 0.        ,\n",
       "        0.04166667, 0.08333333, 0.04166667, 0.04166667, 0.04166667,\n",
       "        0.        , 0.04166667, 0.04166667, 0.04166667, 0.        ,\n",
       "        0.        , 0.        , 0.04166667, 0.04166667, 0.        ,\n",
       "        0.04166667, 0.04166667, 0.04166667, 0.04166667, 0.04166667,\n",
       "        0.04166667, 0.125     , 0.        , 0.        , 0.04166667,\n",
       "        0.        , 0.04166667]),\n",
       " array([0.05263158, 0.05263158, 0.05263158, 0.        , 0.15789474,\n",
       "        0.        , 0.05263158, 0.        , 0.        , 0.        ,\n",
       "        0.05263158, 0.        , 0.        , 0.        , 0.05263158,\n",
       "        0.05263158, 0.05263158, 0.        , 0.        , 0.05263158,\n",
       "        0.10526316, 0.        , 0.        , 0.05263158, 0.        ,\n",
       "        0.        , 0.        , 0.10526316, 0.05263158, 0.        ,\n",
       "        0.05263158, 0.        ]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)#计算训练的文档数目\n",
    "    numWords = len(trainMatrix[0])#计算每篇文档的词条数\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)#文档属于侮辱类的概率\n",
    "    p0Num = np.zeros(numWords); p1Num = np.zeros(numWords)#创建numpy.zeros数组,\n",
    "    p0Denom = 0.0; p1Denom = 0.0                        #分母初始化为0.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:#统计属于侮辱类的条件概率所需的数据，即P(w0|1),P(w1|1),P(w2|1)···\n",
    "            p1Num += trainMatrix[i]\n",
    "            print(p1Num)\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:#统计属于非侮辱类的条件概率所需的数据，即P(w0|0),P(w1|0),P(w2|0)···\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = p1Num/p1Denom#相除        \n",
    "    p0Vect = p0Num/p0Denom          \n",
    "    return p0Vect,p1Vect,pAbusive#返回属于侮辱类的条件概率数组，属于非侮辱类的条件概率数组，文档属于侮辱类的概率\n",
    "trainNB0(trainSet,classVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1], [1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0]] [0, 1, 0, 1, 0, 1]\n",
      "['take', 'not', 'stop', 'licks', 'stupid', 'is', 'him', 'I', 'so', 'mr', 'park', 'love', 'has', 'help', 'garbage', 'food', 'maybe', 'cute', 'please', 'posting', 'dog', 'how', 'problems', 'to', 'dalmation', 'ate', 'my', 'worthless', 'buying', 'steak', 'quit', 'flea']\n",
      "[[0.         0.         0.04166667 0.04166667 0.         0.04166667\n",
      "  0.08333333 0.04166667 0.04166667 0.04166667 0.         0.04166667\n",
      "  0.04166667 0.04166667 0.         0.         0.         0.04166667\n",
      "  0.04166667 0.         0.04166667 0.04166667 0.04166667 0.04166667\n",
      "  0.04166667 0.04166667 0.125      0.         0.         0.04166667\n",
      "  0.         0.04166667]\n",
      " [0.05263158 0.05263158 0.05263158 0.         0.15789474 0.\n",
      "  0.05263158 0.         0.         0.         0.05263158 0.\n",
      "  0.         0.         0.05263158 0.05263158 0.05263158 0.\n",
      "  0.         0.05263158 0.10526316 0.         0.         0.05263158\n",
      "  0.         0.         0.         0.10526316 0.05263158 0.\n",
      "  0.05263158 0.        ]] {0: 0.5, 1: 0.5}\n"
     ]
    }
   ],
   "source": [
    "#说明一步了（希望作者可以补充一下~），p（w0|1） = p(w0∩1)/p(1) = （在类1中w0词汇出现数量/所有词汇的总数量）/（类1中词汇的数量 /所有词汇的总数量）\n",
    "#=>（约掉分母） = (在类1中w0词汇出现的数量)/(类1中词汇的数量)。\n",
    "#并且朴素贝叶斯中 w0,w1,...，wn是独立的，且我们所求为（w在1类中的概率） p(1|w)=p(w|1)p(1)/p(w) p(w)都一样，所以去掉，\n",
    "#所以公式如代码所求。 p(1|w) ==>p（w|1）p(1)为所求\n",
    "def trainNB1(dataSet,classVec):\n",
    "    dataSet=np.array(dataSet).astype(float)\n",
    "    row_length,col_length=dataSet.shape #向量个数和长度\n",
    "    type_counter=Counter(classVec)\n",
    "    Pi_type_dict=dict() #对应不同类型的概率的字典\n",
    "    for k,v in type_counter.items():\n",
    "        Pi_type_dict[k]=v/row_length\n",
    "    type_count_value_list=list(type_counter.values())\n",
    "    Pi_type_keys_list =list(Pi_type_dict.keys())\n",
    "    Pi_type_values_list=list(Pi_type_dict.values()) #类型表\n",
    "    type_total=len(Pi_type_dict) #类型的数量\n",
    "    PXiY=np.zeros((type_total,col_length)) #给每个类型创建一个保存条件概率的向量,每一行对应每一个的类型\n",
    "    #PXiY=np.ones((type_total,col_length))#拉普拉斯修正\n",
    "    #type_words_sum=[2.0]*type_total\n",
    "    type_words_sum=[0]*type_total\n",
    "    for ix in range(row_length):\n",
    "        Pix=Pi_type_keys_list.index(classVec[ix])#对应类型的下标\n",
    "        PXiY[Pix]+=dataSet[ix]#将每一对应的相同类型向量相加，存到PXIY\n",
    "        type_words_sum[Pix]+=sum(dataSet[ix])#求和所有同类型的单词数量\n",
    "    for ix in range(type_total):\n",
    "        # #取对数，防止下溢出         \n",
    "        PXiY[ix]=PXiY[ix]/type_words_sum[ix] #求条件概率：P(X|Y)=S(XY)/该类型所有词汇的数量\n",
    "    return PXiY,Pi_type_dict\n",
    "\n",
    "print(trainSet,classVec)\n",
    "print(mylist)\n",
    "dataItem,Pi_dict=trainNB1(trainSet,classVec)\n",
    "print(dataItem,Pi_dict)\n",
    "\n",
    "# data1=dataItem[0][np.where(dataItem[0]>0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0: 0.0\n",
      "p1: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = reduce(lambda x,y:x*y, vec2Classify * p1Vec) * pClass1 #对应元素相乘\n",
    "    p0 = reduce(lambda x,y:x*y, vec2Classify * p0Vec) * (1.0 - pClass1)\n",
    "    print('p0:',p0)\n",
    "    print('p1:',p1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "testEntry = ['love', 'my', 'dalmation']\n",
    "test=setOfWords2Vec(testEntry,mylist)\n",
    "classifyNB(test,dataItem[0],dataItem[0],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大概率为： 0.00010850694444444444 ，类型： 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00010850694444444444, 0)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classfy_bys(dataSet,classVec,testVec):\n",
    "    max_P=0\n",
    "    max_type=0\n",
    "    type_keys_list=list(classVec.keys()) \n",
    "    type_values_list=list(classVec.values()) \n",
    "    for ix in range(len(dataSet)):\n",
    "        dataItem=dataSet[ix]*testVec\n",
    "        #print(dataSet[ix],dataSet[ix]*testVec)\n",
    "        Item_temp=dataItem[np.where(dataItem>0)]\n",
    "        P=np.prod(Item_temp)*type_values_list[ix]\n",
    "        if len(Item_temp)==0:\n",
    "            P=0\n",
    "        if max_P<P:\n",
    "            max_P=P\n",
    "            max_type=type_keys_list[ix]\n",
    "    print('最大概率为：',max_P,'，类型：',max_type)\n",
    "    return max_P,max_type\n",
    "testEntry = ['love', 'my', 'dalmation']\n",
    "test=setOfWords2Vec(testEntry,mylist)\n",
    "classfy_bys(dataItem,Pi_dict,test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
