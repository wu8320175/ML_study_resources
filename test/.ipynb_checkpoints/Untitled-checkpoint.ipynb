{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "name 'parameters' is used prior to global declaration (cell_name, line 246)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"cell_name\"\u001b[1;36m, line \u001b[1;32m246\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m name 'parameters' is used prior to global declaration\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import struct\n",
    "import copy\n",
    "\n",
    "# 数据集路径\n",
    "dataset_path = Path('./dataset')\n",
    "# 训练图片集路径\n",
    "train_img_path = './dataset/train-images-idx3-ubyte'\n",
    "train_lab_path = './dataset/train-labels-idx1-ubyte'\n",
    "test_img_path = './dataset/t10k-images-idx3-ubyte'\n",
    "test_lab_path = './dataset/t10k-labels-idx1-ubyte'\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "\treturn np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "\texp = np.exp(x-x.max())\n",
    "\treturn exp/exp.sum()\n",
    "dimensions = [28*28,10]\n",
    "activation = [tanh,softmax]\n",
    "distribution=[\n",
    "{\n",
    "\t'b':[0,0]\n",
    "},{\n",
    "\t'b':[0,0],\n",
    "\t'w':[-math.sqrt(6/(dimensions[0]+dimensions[1])),math.sqrt(6/(dimensions[0]+dimensions[1]))]\n",
    "}]\n",
    "\n",
    "# 初始化参数b\n",
    "def init_parameters_b(layer):\n",
    "\tdist = distribution[layer]['b']\n",
    "\treturn np.random.rand(dimensions[layer])*(dist[1]-dist[0])+dist[0]\n",
    "# 初始化参数w\n",
    "def init_parameters_w(layer):\n",
    "\tdist = distribution[layer]['w']\n",
    "\treturn np.random.rand(dimensions[layer-1],dimensions[layer])*(dist[1]-dist[0])+dist[0]\n",
    "\n",
    "#初始化参数方法\n",
    "def init_parameters():\n",
    "\tparameter=[]\n",
    "\tfor i in range(len(distribution)):\n",
    "\t\tlayer_parameter={}\n",
    "\t\tfor j in distribution[i].keys():\n",
    "\t\t\tif j=='b':\n",
    "\t\t\t\tlayer_parameter['b'] = init_parameters_b(i)\n",
    "\t\t\t\tcontinue;\n",
    "\t\t\tif j=='w':\n",
    "\t\t\t\tlayer_parameter['w'] = init_parameters_w(i)\n",
    "\t\t\t\tcontinue\n",
    "\t\tparameter.append(layer_parameter)\n",
    "\treturn parameter\n",
    "\n",
    "# 预测函数\n",
    "def predict(img,init_parameters):\n",
    "\tl0_in = img+parameters[0]['b']\n",
    "\tl0_out = activation[0](l0_in)\n",
    "\tl1_in = np.dot(l0_out,parameters[1]['w'])+parameters[1]['b']\n",
    "\tl1_out = activation[1](l1_in)\n",
    "\treturn l1_out\n",
    "# print(predict(np.random.rand(784),parameters).argmax())\n",
    "# im = np.reshape(np.random.rand(784),(28,28))\n",
    "# plt.imshow(im,cmap='gray')\n",
    "# plt.show()\n",
    "# print(tanh(0.1))\n",
    "# print(init_parameters())\n",
    "# print(softmax(np.array([1,2,3,4])))\n",
    "# print(init_parameters_w(1).shape)\n",
    "\n",
    "\n",
    "# 读16个字节\n",
    "# s = struct.unpack('>4i',train_f.read(16));\n",
    "\n",
    "# 训练50000个，验证10000个，测试10000个\n",
    "train_num = 50000\n",
    "valid_num = 10000\n",
    "test_num = 10000\n",
    "\n",
    "# 读入训练图片集和验证图片集\n",
    "with open(train_img_path,'rb') as f:\n",
    "\tstruct.unpack('>4i',f.read(16))\n",
    "\ttmp_img = np.fromfile(f,dtype = np.uint8).reshape(-1,28*28)\n",
    "\ttrain_img = tmp_img[:train_num]\n",
    "\tvalid_img = tmp_img[train_num:]\n",
    "\n",
    "# 读入测试图片集\n",
    "with open(test_img_path,'rb') as f:\n",
    "\tstruct.unpack('>4i',f.read(16))\n",
    "\ttest_img = np.fromfile(f,dtype = np.uint8).reshape(-1,28*28)\n",
    "\n",
    "# 读入训练标签和验证标签\n",
    "with open(train_lab_path,'rb') as f:\n",
    "\tstruct.unpack('>2i',f.read(8))\n",
    "\ttmp_lab = np.fromfile(f,dtype = np.uint8)\n",
    "\ttrain_lab = tmp_lab[:train_num]\n",
    "\tvalid_lab = tmp_lab[train_num:]\n",
    "\n",
    "# 读入测试标签\n",
    "with open(test_lab_path,'rb') as f:\n",
    "\tstruct.unpack('>2i',f.read(8))\n",
    "\ttest_lab = np.fromfile(f,dtype = np.uint8)\n",
    "\n",
    "# 展示训练图片\n",
    "def show_train(index):\n",
    "\tplt.imshow(train_img[index].reshape(28,28),cmap = 'gray')\n",
    "\tprint('label  = {}'.format(train_lab[index]))\n",
    "\tplt.show()\n",
    "\n",
    "# 展示验证图片\n",
    "def show_valid(index):\n",
    "\tplt.imshow(valid_img[index].reshape(28,28),cmap = 'gray')\n",
    "\tprint('label  = {}'.format(valid_lab[index]))\n",
    "\tplt.show()\n",
    "\n",
    "# 展示测试图片\n",
    "def show_test(index):\n",
    "\tplt.imshow(test_img[index].reshape(28,28),cmap = 'gray')\n",
    "\tprint('label  = {}'.format(test_lab[index]))\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "\n",
    "# softmax导数函数\n",
    "def d_softmax(data):\n",
    "\tsm = softmax(data)\n",
    "\t# diag:对角矩阵  outer：第一个参数挨个乘以第二个参数得到矩阵\n",
    "\treturn np.diag(sm)-np.outer(sm,sm)\n",
    "\n",
    "# tanh导数函数\n",
    "# def d_tanh(data):\n",
    "# \treturn np.diag(1/(np.cosh(data))**2)\n",
    "# tanh导数函数优化：\n",
    "def d_tanh(data):\n",
    "\treturn 1/(np.cosh(data))**2\n",
    "\n",
    "differential = {softmax:d_softmax,tanh:d_tanh}\n",
    "\n",
    "# lab解析函数\n",
    "# 将数解析为某一位置为1的一维矩阵\n",
    "onehot = np.identity(dimensions[-1])\n",
    "\n",
    "# 求平方差函数\n",
    "def sqr_loss(img,lab,parameters):\n",
    "\ty_pred = predict(img,parameters)\n",
    "\ty = onehot[lab]\n",
    "\tdiff = y-y_pred\n",
    "\treturn np.dot(diff,diff)\n",
    "\n",
    "# 计算梯度\n",
    "def grad_parameters(img,lab,init_parameters):\n",
    "\tl0_in = img+parameters[0]['b']\n",
    "\tl0_out = activation[0](l0_in)\n",
    "\tl1_in = np.dot(l0_out,parameters[1]['w'])+parameters[1]['b']\n",
    "\tl1_out = activation[1](l1_in)\n",
    "\t\n",
    "\tdiff = onehot[lab]-l1_out\n",
    "\tact1 = np.dot(differential[activation[1]](l1_in),diff)\n",
    "\n",
    "\tgrad_b1 = -2*act1\n",
    "\tgrad_w1 = -2*np.outer(l0_out,act1)\n",
    "\t# 与上文优化d_tanh有关，将矩阵乘法化为数组乘以矩阵\n",
    "\tgrad_b0 = -2*differential[activation[0]](l0_in)*np.dot(parameters[1]['w'],act1)\n",
    "\n",
    "\treturn {'b1':grad_b1,'w1':grad_w1,'b0':grad_b0}\n",
    "\n",
    "# test b1\n",
    "def test_b1(h):\n",
    "\tfor i in range(10):\n",
    "\t\timg_i = np.random.randint(train_num)\n",
    "\t\ttest_parameters = init_parameters()\n",
    "\t\tderivative = grad_parameters(train_img[img_i],train_lab[img_i],test_parameters)['b1']\n",
    "\t\tvalue1 = sqr_loss(train_img[img_i],train_lab[img_i],test_parameters)\n",
    "\t\ttest_parameters[1]['b'][i]+=h\n",
    "\t\tvalue2 = sqr_loss(train_img[img_i],train_lab[img_i],test_parameters)\n",
    "\t\tprint(derivative[i]-(value2-value1)/h)\n",
    "\n",
    "# test b0\n",
    "def test_b0(h):\n",
    "\tgrad_list = []\n",
    "\tfor i in range(784):\n",
    "\t\timg_i = np.random.randint(train_num)\n",
    "\t\ttest_parameters = init_parameters()\n",
    "\t\tderivative = grad_parameters(train_img[img_i],train_lab[img_i],test_parameters)['b0']\n",
    "\t\tvalue1 = sqr_loss(train_img[img_i],train_lab[img_i],test_parameters)\n",
    "\t\ttest_parameters[0]['b'][i]+=h\n",
    "\t\tvalue2 = sqr_loss(train_img[img_i],train_lab[img_i],test_parameters)\n",
    "\t\tgrad_list.append(derivative[i]-(value2-value1)/h)\n",
    "\treturn grad_list\n",
    "\n",
    "# test w1\n",
    "def test_w1(h):\n",
    "\tgrad_list = []\n",
    "\tfor i in range(784):\n",
    "\t\tfor j in range(10):\n",
    "\t\t\timg_i = np.random.randint(train_num)\n",
    "\t\t\ttest_parameters = init_parameters()\n",
    "\t\t\tderivative = grad_parameters(train_img[img_i],train_lab[img_i],test_parameters)['w1']\n",
    "\t\t\tvalue1 = sqr_loss(train_img[img_i],train_lab[img_i],test_parameters)\n",
    "\t\t\ttest_parameters[1]['w'][i][j]+=h\n",
    "\t\t\tvalue2 = sqr_loss(train_img[img_i],train_lab[img_i],test_parameters)\n",
    "\t\t\tgrad_list.append(derivative[i][j]-(value2-value1)/h)\n",
    "\treturn grad_list\n",
    "\n",
    "def valid_loss(parameters):\n",
    "\tloss_accu = 0\n",
    "\tfor img_i in range(valid_num):\n",
    "\t\tloss_accu+=sqr_loss(valid_img[img_i],valid_lab[img_i],parameters)\n",
    "\treturn loss_accu\n",
    "\n",
    "# valid 集的精确度\n",
    "def valid_accuracy(parameters):\n",
    "\tcorrect = [predict(valid_img[img_i],parameters).argmax()==valid_lab[img_i] for img_i in range(valid_num) ]\n",
    "\tprint(\"validation accuracy:{}\".format(correct.count(True)/len(correct)))\n",
    "\n",
    "# 每组个数\n",
    "batch_size=100\n",
    "\n",
    "def train_batch(current_batch,parameters):\n",
    "\tgrad_accu = grad_parameters(train_img[current_batch*batch_size+0],train_lab[current_batch*batch_size+0],parameters)\n",
    "\tfor img_i in range(1,batch_size):\n",
    "\t\tgrad_tmp = grad_parameters(train_img[current_batch*batch_size+img_i],train_lab[current_batch*batch_size+img_i],parameters)\n",
    "\t\tfor key in grad_accu.keys():\n",
    "\t\t\tgrad_accu[key] += grad_tmp[key]\n",
    "\tfor key in grad_accu.keys():\n",
    "\t\tgrad_accu[key]/=batch_size\n",
    "\treturn grad_accu\n",
    "\n",
    "def combine_parameters(parameters,grad,learn_rate):\n",
    "    parameter_tmp = copy.deepcopy(parameters)\n",
    "    parameter_tmp[0]['b'] -= learn_rate*grad['b0']\n",
    "    parameter_tmp[1]['b'] -= learn_rate*grad['b1']\n",
    "    parameter_tmp[1]['w'] -= learn_rate*grad['w1']\n",
    "    return parameter_tmp\n",
    "\n",
    "def learn_self(learn_rate):\n",
    "    for i in range(train_num//batch_size):\n",
    "        if i%100 == 99:\n",
    "            print(\"running batch {}/{}\".format(i+1,train_num//batch_size))\n",
    "        grad_tmp = train_batch(i,parameters)\n",
    "        global parameters\n",
    "        parameters = combine_parameters(parameters,grad_tmp,learn_rate)\n",
    "        # parameters = init_parameters()\n",
    "# valid_accuracy(parameters)\n",
    "# learn_self(1);\n",
    "# valid_accuracy(parameters)\n",
    "\n",
    "# train_batch(0,parameters)\n",
    "# print(valid_loss(parameters))\n",
    "# valid_accuracy(parameters)\n",
    "\n",
    "# print(np.abs(test_b0(0.000001)).max())\n",
    "# print(grad_parameters(train_img[0],train_lab[0],parameters))\n",
    "# print (sqr_loss(train_img[0],train_lab[0],parameters))\n",
    "# show_train(np.random.randint(train_num))\n",
    "# show_valid(np.random.randint(valid_num))\n",
    "# show_test(np.random.randint(test_num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
